{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ayushs9020/re-creating-protein-neural-nets-from-scratch?scriptVersionId=136117231\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"d80d0548","metadata":{"papermill":{"duration":0.015175,"end_time":"2023-07-08T15:24:45.847465","exception":false,"start_time":"2023-07-08T15:24:45.83229","status":"completed"},"tags":[]},"source":["# Protein Neural Network\n","\n","$Protein$ $Neural$ $Network$ is a type of `deep learning neural network` that is used to `predict` the `properties of proteins`. They are `trained` on a `large dataset` of `protein sequences`/`structures`. The network learns to `identify patterns` in the data that are associated with `particular properties`. $Protein$ $Neural$ $Networks$ have been shown to be `very effective` at `predicting protein properties`. In many cases, they `outperform traditional` methods that rely on `hand-crafted features`\n","\n","<img src = \"https://scontent.fjai2-1.fna.fbcdn.net/v/t1.6435-9/37298434_654990401529724_543197132539035648_n.jpg?_nc_cat=102&ccb=1-7&_nc_sid=730e14&_nc_ohc=7hJcasqo1PkAX8fs55i&_nc_ht=scontent.fjai2-1.fna&oh=00_AfBLiYMGoJgSMX0w-kd690Y2GvRX4ga07ER2OXTSeH7o5w&oe=64CE544A\" width = 400>\n","\n","\n","# 1 | Basic Terminologies 💻\n","\n","* $Structure$ $of$ $a$ $Protein$\n","* $Gene$ $Ontology$ $(GO)$\n","* $Principal$ $Component$ $Analaysis$\n","* $Transformers$\n","* $Categorical$ $Cross$ $Entropy$\n","* $Adaptive$ $Moment$ $Estimation$ $ADAM$\n","\n","## 1.1 | Structure of a Protein\n","\n","So what is a actually a **Protein...?**\n","\n","First of all lets understand the structure of an **Atom**\n","\n","<img src = \"https://www.sciencefacts.net/wp-content/uploads/2020/11/Parts-of-an-Atom-Diagram.jpg\"  width = 300>\n","\n","There is a really good image I found of the `structure of atom`. Though there are many debates on the structure like this, but this `model is accepted universaly at this moment`.\n","\n","In the centre we have the `Neucleus`. The `Neucleus` is made up of $2$ more structures named as `Neutron` and `Proton`. A `Proton` is `positively charged element` and a `Neutron` is a `neutral charged element`. A `Electron`, `negatively charged element`, `orbits` this `Neucleus` at some `distance apart`.\n","\n","The `more we increase the number` of `Electrons` and `Protons`. The `bigger the atoms becomes`.\n","\n","There are `different shells` where the `Electrons reside`. The `more closer the shell` is, the `less Electrons` it contrains. There are mainly $4$ shells. \n","\n","|||\n","|---|---\n","|$K$|$2$\n","|$L$|$8$\n","|$M$|$18$\n","|$N$|$32$\n","\n","Once an atom `fills its outer most shell` with `Electrons`. It becomes `stable atom` and try to `refuse any donation` or `recieve of extra atom`.\n","\n","<img src = \"https://cdn1.byjus.com/wp-content/uploads/2022/01/word-image128.png\" width = 400>\n","\n","`Different atoms combine` to `share Electrons` and become `stable Molecules` \n","\n","<img src = \"https://www.astrochem.org/sci_img/Amino_Acid_Structure.jpg\" width = 300>\n","\n","A `Amino Acid` is made up of mainly $4$ different atoms\n","`[H , C , O , N]`. \n","\n","<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/L-amino_acid_structure.svg/1200px-L-amino_acid_structure.svg.png\" alt = \"Bro use Light Theme\" width = 300 >\n","\n","We also have a free `Electron Pair` of `Carbon` in this molecule, we call this as a `Side Chain` which can be of different types. Basically this `Side Chain` provide the flexibility to make `different types` of `Amino Acids`. This flexibilty allows for $20$ `different` `Amino Acids` \n","\n","When we join `Amino Acids` with `peptide bonds`, we get `Proteins`. Conncecting different types of `Amino Acids` ends up in different types of `Proteins`.\n","\n","## 1.2 | Gene Ontology (GO)\n","\n","$Gene$ $Ontology$ $(GO)$ is a `controlled vocabulary` that `describes the functions of genes` and gene products. It is constantly being updated as new information becomes available.\n","\n","There are mainly $3$ `Ontologies`\n","\n","||||\n","|---|---|---\n","|$Biological$ $Process$|Describes the `biological processes`|A gene product might be involved in the process of `cell cycle`/`signal transduction.`\n","|$Cellular$ $Component$|Describes the `cellular components`|A gene product might be located in the `nucleus`/`cytoplasm`.\n","|$Molecular$ $Function$|Describes the `molecular functions`|A gene product might be involved in the `catalysis of a reaction`/`binding of a molecule`.\n","\n","**[Gene Ontology Documentation](http://geneontology.org/docs/ontology-documentation/)**\n","\n","## 1.3 | Principal Component Analysis\n","\n","<img src = \"https://media.makeameme.org/created/pca-the-cause.jpg\" width = 400>\n","\n","$Principal$ $Component$ $Analysis$ $PCA$ is a `statistical procedure` that uses an `orthogonal transformation` to convert a set of `correlated variables` into a set of `uncorrelated variables` called principal components.\n","\n","First we compute the Covariance Matrix with the formula \n","\n","$$Cov_{x , y} = \\frac{\\sum(x_i - x_{mean})(y_i - y_{mean})}{N-1}$$\n","\n","Then we find the Eign Values and Eign Vectors \n","\n","## 1.4 | Transformers \n","\n","A $Transformer$ is a `deep learning architecture` that relies on the `attention mechanism`. It is notable for requiring `less training time`. The model takes in `tokenized` `byte pair encoding` `input` tokens, and at each layer, `contextualizes each token` with other (unmasked) input tokens in `parallel` via attention mechanism.\n","\n","* $Attention$ $Mechanism$- The attention mechanism allows the transformer to `learn long-range dependencies` between `tokens` in a sequence. \n","* $Self-Attention$ - Self-attention is a special type of attention that allows the transformer to `attend to itself`. This means that the transformer can learn relationships between different parts of the same sequence.\n","* $Encoder-Decoder$ $Architecture$ - The transformer is an `Encoder-Decoder` architecture. This means that the transformer has two parts\n","* * Encoder - takes the input sequence and produces a sequence of hidden states\n","* * Decoder - takes these hidden states and produces the output sequence.\n","\n","$$a = Softmax(\\frac{KQ^T}{\\sqrt{d_k}})$$\n","\n","<img src = \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width = 400>\n","\n","## 1.5 | Categorical Cross Entropy \n","\n","$Loss$ is a `measure` of `how well a model is performing` on a given task. It is `calculated` by `comparing` the `model's predictions` to the `ground truth labels`. The `lower the loss`, the `better the model` is performing.\n","\n","Here we will be using `Cross Entropy Loss` , a loss function that `measures` the `difference` between the `model's predicted probability distribution` and the `ground truth distribution`.\n","\n","## 1.6 | Adaptive Moment Estimation \n","\n","An $Optimizer$ is an `algorithm` or function that `updates the weights and biases` of a neural network in order to `minimize a loss function`. \n","\n","Here we will be using the `Adam Optimizer`\n","\n","$Adam$ is an $Adaptive$ $Learning$ $Rate$ method, which works by `maintaining two moving averages of the gradients`\n","* $Mean$ - Calculate the `momentum term`, which helps to `prevent` the `optimizer` from `getting stuck in local minima`\n","* $Variance$ - Calculate the `learning rate`, which is `adjusted based on the magnitude of the gradients`.\n","\n","$$m_t = \\beta_1 * m_{t - 1} + (1 - \\beta_1) * w_t$$\n","\n","$$v_t = \\beta_2 * m_{t - 1} + (1 - \\beta_2) * w_t$$\n","\n","$$m_t = \\frac{m_t}{1 - \\beta_1^t}$$\n","\n","$$v_t = \\frac{v_t}{1 - \\beta_2^t}$$\n","\n","$$w_{t+1} = w_t - \\frac{n}{\\sqrt{v_t + e}} * m_t$$\n","\n","# 2 | Data 📊\n","\n","The `goal` of this competition is to `predict the function of a set of proteins`. We will `develop a model trained` on the `amino-acid sequences` of the `proteins and on other data`. Our work `will help researchers` better `understand the function of proteins`, which is `important for discovering` `how cells, tissues, and organs work`.\n"]},{"cell_type":"code","execution_count":1,"id":"87fd89e0","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-07-08T15:24:45.873595Z","iopub.status.busy":"2023-07-08T15:24:45.873201Z","iopub.status.idle":"2023-07-08T15:24:46.010564Z","shell.execute_reply":"2023-07-08T15:24:46.009751Z"},"papermill":{"duration":0.153413,"end_time":"2023-07-08T15:24:46.013168","exception":false,"start_time":"2023-07-08T15:24:45.859755","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd \n","import re\n","from Bio import SeqIO"]},{"cell_type":"markdown","id":"e012ff9e","metadata":{"papermill":{"duration":0.012118,"end_time":"2023-07-08T15:24:46.037741","exception":false,"start_time":"2023-07-08T15:24:46.025623","status":"completed"},"tags":[]},"source":["The $Training$ $Set$ contains all `proteins with annotated terms` that have been validated by \n","* $Experimental$\n","* $High-Throughput$ $Evidence$\n","* [$Traceable$ $Author$ $Statement$](https://wiki.geneontology.org/index.php/Traceable_Author_Statement_(TAS)#:~:text=The%20TAS%20evidence%20code%20covers,annotations%20come%20from%20review%20articles.)\n","* [$Inferred$ $by$ $Curator$ $(IC)$](https://wiki.geneontology.org/Inferred_by_Curator_(IC)) \n","\n","**Any other sources of Data are allowed**\n","\n","### 2.1.1.1 | Go-Basic.obo\n","\n","The $Ontology$ data is in the `file go-basic.obo`. This file is in $OBO$ `Biology-Oriented Language`. The nodes in `the graph are indexed` by the `term name`\n","```\n","subontology_roots = {'BPO':'GO:0008150',\n","                     'CCO':'GO:0005575',\n","                     'MFO':'GO:0003674'}\n","```"]},{"cell_type":"code","execution_count":2,"id":"8eeafd19","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-08T15:24:46.064026Z","iopub.status.busy":"2023-07-08T15:24:46.063619Z","iopub.status.idle":"2023-07-08T15:24:50.4717Z","shell.execute_reply":"2023-07-08T15:24:50.470622Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":4.424539,"end_time":"2023-07-08T15:24:50.474686","exception":false,"start_time":"2023-07-08T15:24:46.050147","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[Term]\n","id: GO:0000001\n","name: mitochondrion inheritance\n","namespace: biological_process\n","def: \"The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton.\" [GOC:mcc, PMID:10873824, PMID:11389764]\n","synonym: \"mitochondrial inheritance\" EXACT []\n","is_a: GO:0048308 ! organelle inheritance\n","is_a: GO:0048311 ! mitochondrion distribution\n","\n"]}],"source":["with open('/kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo') as file :\n","    \n","    content = file.read()\n","    stanzas =  re.findall(r'\\[Term\\][\\s\\S]*?(?=\\n\\[|$)' , content)\n","    \n","print(stanzas[0])"]},{"cell_type":"markdown","id":"8bc87177","metadata":{"papermill":{"duration":0.012009,"end_time":"2023-07-08T15:24:50.499377","exception":false,"start_time":"2023-07-08T15:24:50.487368","status":"completed"},"tags":[]},"source":["### 2.1.1.2 | Training Sequences.fasta\n","\n","This file contains only `sequences` for `proteins` with `annotations` in the dataset `labeled proteins`.\n","\n","This files are in `FASTA` format. \n","\n","This file contains . To obtain the full set of protein sequences for unlabeled proteins, the Swiss-Prot and TrEMBL databases can be found here."]},{"cell_type":"code","execution_count":3,"id":"cb658a88","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:24:50.526284Z","iopub.status.busy":"2023-07-08T15:24:50.525881Z","iopub.status.idle":"2023-07-08T15:24:50.554415Z","shell.execute_reply":"2023-07-08T15:24:50.553353Z"},"papermill":{"duration":0.044922,"end_time":"2023-07-08T15:24:50.556846","exception":false,"start_time":"2023-07-08T15:24:50.511924","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["<Bio.SeqIO.FastaIO.FastaIterator at 0x7a56e4a24850>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["fasta_seq = SeqIO.parse(open(\"/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\") ,'fasta')\n","\n","fasta_seq"]},{"cell_type":"markdown","id":"d7648ad9","metadata":{"papermill":{"duration":0.012333,"end_time":"2023-07-08T15:24:50.5823","exception":false,"start_time":"2023-07-08T15:24:50.569967","status":"completed"},"tags":[]},"source":["### 2.1.1.3 | Train Terms.tsv\n","\n","This file contains the list of annotated terms `ground truth` for the proteins in `train_sequences.fasta`. "]},{"cell_type":"code","execution_count":4,"id":"f7da9d10","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-08T15:24:50.608885Z","iopub.status.busy":"2023-07-08T15:24:50.608468Z","iopub.status.idle":"2023-07-08T15:24:54.552613Z","shell.execute_reply":"2023-07-08T15:24:54.551408Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":3.960216,"end_time":"2023-07-08T15:24:54.555039","exception":false,"start_time":"2023-07-08T15:24:50.594823","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EntryID</th>\n","      <th>term</th>\n","      <th>aspect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0008152</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0034655</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0072523</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0044270</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0006753</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5363858</th>\n","      <td>X5L565</td>\n","      <td>GO:0050649</td>\n","      <td>MFO</td>\n","    </tr>\n","    <tr>\n","      <th>5363859</th>\n","      <td>X5L565</td>\n","      <td>GO:0016491</td>\n","      <td>MFO</td>\n","    </tr>\n","    <tr>\n","      <th>5363860</th>\n","      <td>X5M5N0</td>\n","      <td>GO:0005515</td>\n","      <td>MFO</td>\n","    </tr>\n","    <tr>\n","      <th>5363861</th>\n","      <td>X5M5N0</td>\n","      <td>GO:0005488</td>\n","      <td>MFO</td>\n","    </tr>\n","    <tr>\n","      <th>5363862</th>\n","      <td>X5M5N0</td>\n","      <td>GO:0003674</td>\n","      <td>MFO</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5363863 rows × 3 columns</p>\n","</div>"],"text/plain":["            EntryID        term aspect\n","0        A0A009IHW8  GO:0008152    BPO\n","1        A0A009IHW8  GO:0034655    BPO\n","2        A0A009IHW8  GO:0072523    BPO\n","3        A0A009IHW8  GO:0044270    BPO\n","4        A0A009IHW8  GO:0006753    BPO\n","...             ...         ...    ...\n","5363858      X5L565  GO:0050649    MFO\n","5363859      X5L565  GO:0016491    MFO\n","5363860      X5M5N0  GO:0005515    MFO\n","5363861      X5M5N0  GO:0005488    MFO\n","5363862      X5M5N0  GO:0003674    MFO\n","\n","[5363863 rows x 3 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv(\"/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\" , sep = \"\\t\")"]},{"cell_type":"markdown","id":"7e031a94","metadata":{"papermill":{"duration":0.012555,"end_time":"2023-07-08T15:24:54.581496","exception":false,"start_time":"2023-07-08T15:24:54.568941","status":"completed"},"tags":[]},"source":["The first column indicates the `protein's UniProt accession ID`, the second is the `GO term ID`, and the third indicates in which `ontology the term appears`.\n","\n","### 2.1.1.4 | Train Taxonomy.tsv\n","\n","This file contains the list of `proteins and the species to which they belong`, represented by a `taxonomic identifier` `taxon ID` number."]},{"cell_type":"code","execution_count":5,"id":"2dc77c8e","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-08T15:24:54.609611Z","iopub.status.busy":"2023-07-08T15:24:54.608721Z","iopub.status.idle":"2023-07-08T15:24:54.733067Z","shell.execute_reply":"2023-07-08T15:24:54.731962Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.140646,"end_time":"2023-07-08T15:24:54.735397","exception":false,"start_time":"2023-07-08T15:24:54.594751","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EntryID</th>\n","      <th>taxonomyID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q8IXT2</td>\n","      <td>9606</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Q04418</td>\n","      <td>559292</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A8DYA3</td>\n","      <td>7227</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Q9UUI3</td>\n","      <td>284812</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Q57ZS4</td>\n","      <td>185431</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>142241</th>\n","      <td>Q5TD07</td>\n","      <td>9606</td>\n","    </tr>\n","    <tr>\n","      <th>142242</th>\n","      <td>A8BB17</td>\n","      <td>7955</td>\n","    </tr>\n","    <tr>\n","      <th>142243</th>\n","      <td>A0A2R8QBB1</td>\n","      <td>7955</td>\n","    </tr>\n","    <tr>\n","      <th>142244</th>\n","      <td>P0CT72</td>\n","      <td>284812</td>\n","    </tr>\n","    <tr>\n","      <th>142245</th>\n","      <td>Q9NZ43</td>\n","      <td>9606</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>142246 rows × 2 columns</p>\n","</div>"],"text/plain":["           EntryID  taxonomyID\n","0           Q8IXT2        9606\n","1           Q04418      559292\n","2           A8DYA3        7227\n","3           Q9UUI3      284812\n","4           Q57ZS4      185431\n","...            ...         ...\n","142241      Q5TD07        9606\n","142242      A8BB17        7955\n","142243  A0A2R8QBB1        7955\n","142244      P0CT72      284812\n","142245      Q9NZ43        9606\n","\n","[142246 rows x 2 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv(\"/kaggle/input/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv\" , sep = \"\\t\")"]},{"cell_type":"markdown","id":"db6f8311","metadata":{"papermill":{"duration":0.013464,"end_time":"2023-07-08T15:24:54.762299","exception":false,"start_time":"2023-07-08T15:24:54.748835","status":"completed"},"tags":[]},"source":["### 2.1.1.5 | IA.txt\n","\n","IA.txt contains the information accretion (weights) for each GO term. These weights are used to compute weighted precision and recall, as described in the Evaluation section. \n","\n","## 2.1.2 | Test Set\n","\n","The $Test$ $Set$ is `unknown at the beginning` of the competition. It will contain `protein sequences` `their functions` from the `test superset` that `gained experimental annotations` between the `submission-deadline` and the `time of evaluation`.\n","\n","# 3 | PyTorch DataLoader ⚙️"]},{"cell_type":"code","execution_count":6,"id":"aea696a9","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-07-08T15:24:54.796427Z","iopub.status.busy":"2023-07-08T15:24:54.795111Z","iopub.status.idle":"2023-07-08T15:25:09.031705Z","shell.execute_reply":"2023-07-08T15:25:09.030811Z"},"papermill":{"duration":14.254981,"end_time":"2023-07-08T15:25:09.034315","exception":false,"start_time":"2023-07-08T15:24:54.779334","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["import numpy as np\n","from transformers import BertModel, BertTokenizer\n","import torch\n","from torch.utils.data import Dataset"]},{"cell_type":"markdown","id":"a6622880","metadata":{"papermill":{"duration":0.012648,"end_time":"2023-07-08T15:25:09.060143","exception":false,"start_time":"2023-07-08T15:25:09.047495","status":"completed"},"tags":[]},"source":["It this point we will be making $2$ DataLoaders\n","\n","* $Multi$ $Layer$ $Perceptron$ $DataLoader$ - Highly inspired by **[Henri Upton](https://www.kaggle.com/henriupton)=>[ProteiNet 🧬 PyTorch+EMS2/T5/ProtBERT Embeddings](https://www.kaggle.com/code/henriupton/proteinet-pytorch-ems2-t5-protbert-embeddings)**\n","* $Single$ $Layer$ $Perceptron$ $DataLoader$ - Inspired by **[TEMPROT: protein function annotation using transformers embeddings and homology search](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-023-05375-0)** \n","\n","# 3.1 | PytorchData Loader 🧠\n","\n","First we will make a simple class..."]},{"cell_type":"code","execution_count":7,"id":"bdad574c","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:09.088024Z","iopub.status.busy":"2023-07-08T15:25:09.087348Z","iopub.status.idle":"2023-07-08T15:25:09.091383Z","shell.execute_reply":"2023-07-08T15:25:09.090708Z"},"papermill":{"duration":0.020419,"end_time":"2023-07-08T15:25:09.093526","exception":false,"start_time":"2023-07-08T15:25:09.073107","status":"completed"},"tags":[]},"outputs":[],"source":["class Pytorch_Dataset(Dataset):pass"]},{"cell_type":"markdown","id":"9bc9da86","metadata":{"papermill":{"duration":0.012822,"end_time":"2023-07-08T15:25:09.119679","exception":false,"start_time":"2023-07-08T15:25:09.106857","status":"completed"},"tags":[]},"source":["Now we will load the `Embedments` for the data \n","\n","Here we will be using the combination of `T5`/`Prot-BERT`\n","\n","Embeds are like representation of a non-numercial elemenet to a list of numerical elemenet. "]},{"cell_type":"code","execution_count":8,"id":"34ab923f","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:09.147611Z","iopub.status.busy":"2023-07-08T15:25:09.147191Z","iopub.status.idle":"2023-07-08T15:25:09.15297Z","shell.execute_reply":"2023-07-08T15:25:09.151862Z"},"papermill":{"duration":0.022628,"end_time":"2023-07-08T15:25:09.155376","exception":false,"start_time":"2023-07-08T15:25:09.132748","status":"completed"},"tags":[]},"outputs":[],"source":["class Pytorch_Dataset(Dataset):\n","    \n","    def __init__(self):\n","        \n","        super(Pytorch_Dataset).__init__()\n","        \n","        self.embeds = np.load(\"/kaggle/input/t5embeds/test_embeds.npy\")"]},{"cell_type":"markdown","id":"66d01781","metadata":{"papermill":{"duration":0.012683,"end_time":"2023-07-08T15:25:09.181236","exception":false,"start_time":"2023-07-08T15:25:09.168553","status":"completed"},"tags":[]},"source":["In case you wanna see, what these `embeds`looks like, open the below hidden cells\n","\n","At this point we will only consider the first $10,000$ values"]},{"cell_type":"code","execution_count":9,"id":"4de52404","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-07-08T15:25:09.209709Z","iopub.status.busy":"2023-07-08T15:25:09.208596Z","iopub.status.idle":"2023-07-08T15:25:09.532182Z","shell.execute_reply":"2023-07-08T15:25:09.531394Z"},"papermill":{"duration":0.340224,"end_time":"2023-07-08T15:25:09.53451","exception":false,"start_time":"2023-07-08T15:25:09.194286","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(array([[-0.05582625, -0.04506441, -0.2390366 , ...,  0.14752068,\n","          0.19537698,  0.01960445],\n","        [-0.05582625, -0.04506442, -0.2390366 , ...,  0.14752068,\n","          0.19537698,  0.01960445],\n","        [-0.05582625, -0.0450644 , -0.2390366 , ...,  0.14752068,\n","          0.19537698,  0.01960445],\n","        ...,\n","        [-0.05582625, -0.04506441, -0.2390366 , ...,  0.14752068,\n","          0.19537698,  0.01960445],\n","        [-0.05582625, -0.04506441, -0.2390366 , ...,  0.14752068,\n","          0.19537698,  0.01960445],\n","        [-0.05582625, -0.04506441, -0.2390366 , ...,  0.14752068,\n","          0.19537698,  0.01960445]], dtype=float32),\n"," (9999, 1024))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["embeds = np.load(\"/kaggle/input/cafa-sample-embeddings/Embeddings/T5-Prot-BERT BFD/Train/Embeds.npy\")\n","\n","embeds , embeds.shape"]},{"cell_type":"markdown","id":"2809978d","metadata":{"papermill":{"duration":0.013185,"end_time":"2023-07-08T15:25:09.561354","exception":false,"start_time":"2023-07-08T15:25:09.548169","status":"completed"},"tags":[]},"source":["For the `training` we need the `targets`, which we will take from the `train_targets_top500.npy`."]},{"cell_type":"code","execution_count":10,"id":"7759abb5","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:09.590841Z","iopub.status.busy":"2023-07-08T15:25:09.589967Z","iopub.status.idle":"2023-07-08T15:25:09.596504Z","shell.execute_reply":"2023-07-08T15:25:09.595689Z"},"papermill":{"duration":0.023541,"end_time":"2023-07-08T15:25:09.59872","exception":false,"start_time":"2023-07-08T15:25:09.575179","status":"completed"},"tags":[]},"outputs":[],"source":["class Pytorch_Dataset(Dataset):\n","    \n","    def __init__(self , datatype):\n","        \n","        super(Pytorch_Dataset).__init__()\n","        \n","        self.datatype  = datatype\n","        \n","        self.embeds = np.load(\"/kaggle/input/cafa-sample-embeddings/Embeddings/T5-Prot-BERT BFD/Train/Embeds.npy\")\n","        \n","        if self.datatype == \"train\" : \n","            \n","            self.targets = np.load(\"/kaggle/input/train-targets-top500/train_targets_top500.npy\")[:self.embeds.shape[0]]"]},{"cell_type":"markdown","id":"04a11770","metadata":{"papermill":{"duration":0.012735,"end_time":"2023-07-08T15:25:09.624899","exception":false,"start_time":"2023-07-08T15:25:09.612164","status":"completed"},"tags":[]},"source":["These are our targets"]},{"cell_type":"code","execution_count":11,"id":"49700312","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-07-08T15:25:09.65326Z","iopub.status.busy":"2023-07-08T15:25:09.652116Z","iopub.status.idle":"2023-07-08T15:25:13.562116Z","shell.execute_reply":"2023-07-08T15:25:13.561213Z"},"papermill":{"duration":3.926571,"end_time":"2023-07-08T15:25:13.564432","exception":false,"start_time":"2023-07-08T15:25:09.637861","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([[0., 1., 0., ..., 0., 0., 0.],\n","       [1., 1., 1., ..., 0., 1., 1.],\n","       [1., 1., 1., ..., 0., 0., 0.],\n","       ...,\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [1., 1., 1., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["targets = np.load(\"/kaggle/input/train-targets-top500/train_targets_top500.npy\")[:embeds.shape[0]]\n","\n","targets"]},{"cell_type":"markdown","id":"5f76a343","metadata":{"papermill":{"duration":0.013309,"end_time":"2023-07-08T15:25:13.591198","exception":false,"start_time":"2023-07-08T15:25:13.577889","status":"completed"},"tags":[]},"source":["Now we will just apply some `Getters` in the class "]},{"cell_type":"code","execution_count":12,"id":"07b2b7d7","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:13.619545Z","iopub.status.busy":"2023-07-08T15:25:13.619189Z","iopub.status.idle":"2023-07-08T15:25:13.627912Z","shell.execute_reply":"2023-07-08T15:25:13.626779Z"},"papermill":{"duration":0.025662,"end_time":"2023-07-08T15:25:13.630257","exception":false,"start_time":"2023-07-08T15:25:13.604595","status":"completed"},"tags":[]},"outputs":[],"source":["class Pytorch_Dataset(Dataset):\n","\n","    def __init__(self , datatype ):\n","        super(Pytorch_Dataset).__init__()\n","\n","        self.datatype = datatype\n","        self.embeds = np.load(\"/kaggle/input/cafa-sample-embeddings/Embeddings/T5-Prot-BERT BFD/Train/Embeds.npy\")\n","        \n","        if datatype == \"train\":\n","\n","            self.targets = np.load(\"/kaggle/input/train-targets-top500/train_targets_top500.npy\")[:self.embeds.shape[0]]\n","            \n","    def __len__(self):return self.targets.shape[0]\n","\n","    def __getitem__(self , index):\n","\n","        r_embed = torch.tensor(self.embeds[index] , dtype = torch.float32)\n","\n","        if self.datatype == \"train\":\n","\n","            r_targets = torch.tensor(self.targets[index], dtype = torch.float32)\n","\n","            return r_embed, r_targets\n","\n","        return r_embed"]},{"cell_type":"code","execution_count":13,"id":"b5c89f15","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:13.659061Z","iopub.status.busy":"2023-07-08T15:25:13.658703Z","iopub.status.idle":"2023-07-08T15:25:13.951278Z","shell.execute_reply":"2023-07-08T15:25:13.95031Z"},"papermill":{"duration":0.310065,"end_time":"2023-07-08T15:25:13.953973","exception":false,"start_time":"2023-07-08T15:25:13.643908","status":"completed"},"tags":[]},"outputs":[],"source":["train_data = Pytorch_Dataset(datatype = \"train\")"]},{"cell_type":"code","execution_count":14,"id":"805a86ec","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:13.983458Z","iopub.status.busy":"2023-07-08T15:25:13.983088Z","iopub.status.idle":"2023-07-08T15:25:13.989058Z","shell.execute_reply":"2023-07-08T15:25:13.988057Z"},"papermill":{"duration":0.023583,"end_time":"2023-07-08T15:25:13.99144","exception":false,"start_time":"2023-07-08T15:25:13.967857","status":"completed"},"tags":[]},"outputs":[],"source":["train_dataloader = torch.utils.data.DataLoader(train_data , batch_size = 128 , shuffle = True)"]},{"cell_type":"markdown","id":"cb659c95","metadata":{"papermill":{"duration":0.013295,"end_time":"2023-07-08T15:25:14.018361","exception":false,"start_time":"2023-07-08T15:25:14.005066","status":"completed"},"tags":[]},"source":["# 4 | Model Setup 🤖️ \n","\n","Now we will train Simple models, and see the results\n","\n","# 4.1 | Multi Layer Perceptron 🧠\n","\n","A `MultiLayerPerceptron` is a simple model, which consists of just few perceptrons/nodes/neurons connected with each other. \n","\n","As we have choosen for the `top-500` targets, our last layer of the network will contain 500 perceptrons, "]},{"cell_type":"code","execution_count":15,"id":"19c2efcc","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.047056Z","iopub.status.busy":"2023-07-08T15:25:14.046652Z","iopub.status.idle":"2023-07-08T15:25:14.057867Z","shell.execute_reply":"2023-07-08T15:25:14.057022Z"},"papermill":{"duration":0.028044,"end_time":"2023-07-08T15:25:14.059884","exception":false,"start_time":"2023-07-08T15:25:14.03184","status":"completed"},"tags":[]},"outputs":[],"source":["class MultiLayerPerceptron(torch.nn.Module):\n","\n","    def __init__(self):\n","      \n","        super(MultiLayerPerceptron, self).__init__()\n","\n","        # l_1 = Linear Layer 1 \n","        # a_1 = Activation Layer 1\n","\n","        self.l_1 = torch.nn.Linear(1024 , 1000)\n","        self.a_1 = torch.nn.ReLU()\n","\n","        self.l_2 = torch.nn.Linear(1000 , 900)\n","        self.a_2 = torch.nn.ReLU()\n","        \n","        self.l_3 = torch.nn.Linear(900 , 800)\n","        self.a_3 = torch.nn.ReLU()\n","        \n","        self.l_4 = torch.nn.Linear(800 , 700)\n","        self.a_4 = torch.nn.ReLU()\n","        \n","        self.l_5 = torch.nn.Linear(700 , 600)\n","        self.a_5 = torch.nn.ReLU()\n","        \n","        self.l_6 = torch.nn.Linear(600 , 500)\n","    \n","    def forward(self, x):\n","        \n","        x = self.l_1(x)\n","        x = self.a_1(x)\n","\n","        x = self.l_2(x)\n","        x = self.a_2(x)\n","        \n","        x = self.l_3(x)\n","        x = self.a_3(x)\n","        \n","        x = self.l_4(x)\n","        x = self.a_4(x)\n","        \n","        x = self.l_5(x)\n","        x = self.a_5(x)\n","        \n","        x = self.l_6(x)\n","        \n","        return x"]},{"cell_type":"markdown","id":"14cde242","metadata":{"papermill":{"duration":0.01379,"end_time":"2023-07-08T15:25:14.087393","exception":false,"start_time":"2023-07-08T15:25:14.073603","status":"completed"},"tags":[]},"source":["I cannot use `Kaggle GPUs` for some reason (I dont know why), so I will be commenting the `cuda` parts "]},{"cell_type":"code","execution_count":16,"id":"25afd28b","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.116368Z","iopub.status.busy":"2023-07-08T15:25:14.115972Z","iopub.status.idle":"2023-07-08T15:25:14.192091Z","shell.execute_reply":"2023-07-08T15:25:14.191058Z"},"papermill":{"duration":0.093745,"end_time":"2023-07-08T15:25:14.194798","exception":false,"start_time":"2023-07-08T15:25:14.101053","status":"completed"},"tags":[]},"outputs":[],"source":["MLP = MultiLayerPerceptron()\n","\n","# MLP = MultiLayerPerceptron().to(\"cuda\")"]},{"cell_type":"markdown","id":"bf0b44f9","metadata":{"papermill":{"duration":0.013205,"end_time":"2023-07-08T15:25:14.221848","exception":false,"start_time":"2023-07-08T15:25:14.208643","status":"completed"},"tags":[]},"source":["# 4.2 | Single Layer Perceptron 🧠\n","\n","A $Single$ $Layer$ $Perceptron$ is a `Neural Network Architechture` that consists of $1$ `Hidden Layer`. According to the `Paper`, we should  be having $1$ `Hidden Layer` with $1,000$ `Perceptrons` in it. "]},{"cell_type":"code","execution_count":17,"id":"b4fff9c5","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.251557Z","iopub.status.busy":"2023-07-08T15:25:14.250636Z","iopub.status.idle":"2023-07-08T15:25:14.258027Z","shell.execute_reply":"2023-07-08T15:25:14.25723Z"},"papermill":{"duration":0.024498,"end_time":"2023-07-08T15:25:14.260208","exception":false,"start_time":"2023-07-08T15:25:14.23571","status":"completed"},"tags":[]},"outputs":[],"source":["class SingleLayerPerceptron(torch.nn.Module):\n","\n","    def __init__(self):\n","        \n","        super(SingleLayerPerceptron, self).__init__()\n","\n","        self.linear1 = torch.nn.Linear(1024, 1012)\n","        self.activation1 = torch.nn.ReLU()\n","        \n","        self.linear2 = torch.nn.Linear(1012, 500)\n","\n","    def forward(self, x):\n","        \n","        x = self.linear1(x)\n","        x = self.activation1(x)\n","        \n","        x = self.linear2(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":18,"id":"b4b48f05","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.289419Z","iopub.status.busy":"2023-07-08T15:25:14.28871Z","iopub.status.idle":"2023-07-08T15:25:14.312241Z","shell.execute_reply":"2023-07-08T15:25:14.311296Z"},"papermill":{"duration":0.040858,"end_time":"2023-07-08T15:25:14.314888","exception":false,"start_time":"2023-07-08T15:25:14.27403","status":"completed"},"tags":[]},"outputs":[],"source":["SLP = SingleLayerPerceptron()\n","\n","# SLP = SingleLayerPerceptron().to(\"cuda\")"]},{"cell_type":"markdown","id":"c8648748","metadata":{"papermill":{"duration":0.013021,"end_time":"2023-07-08T15:25:14.341707","exception":false,"start_time":"2023-07-08T15:25:14.328686","status":"completed"},"tags":[]},"source":["## Loss Function 📉\n","\n","Here we will be using `Categorical Cross Entropy`"]},{"cell_type":"code","execution_count":19,"id":"7a757e20","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.3699Z","iopub.status.busy":"2023-07-08T15:25:14.369489Z","iopub.status.idle":"2023-07-08T15:25:14.374649Z","shell.execute_reply":"2023-07-08T15:25:14.373515Z"},"papermill":{"duration":0.021997,"end_time":"2023-07-08T15:25:14.377188","exception":false,"start_time":"2023-07-08T15:25:14.355191","status":"completed"},"tags":[]},"outputs":[],"source":["CrossEntropy = torch.nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"09fc66a9","metadata":{"papermill":{"duration":0.013224,"end_time":"2023-07-08T15:25:14.403974","exception":false,"start_time":"2023-07-08T15:25:14.39075","status":"completed"},"tags":[]},"source":["## Optimizer 💡\n","\n","Here we will be using `Adam`"]},{"cell_type":"code","execution_count":20,"id":"97e8754d","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.432771Z","iopub.status.busy":"2023-07-08T15:25:14.432365Z","iopub.status.idle":"2023-07-08T15:25:14.437787Z","shell.execute_reply":"2023-07-08T15:25:14.436582Z"},"papermill":{"duration":0.022332,"end_time":"2023-07-08T15:25:14.440075","exception":false,"start_time":"2023-07-08T15:25:14.417743","status":"completed"},"tags":[]},"outputs":[],"source":["optimizer = torch.optim.Adam(MLP.parameters(), lr = 0.0005)"]},{"cell_type":"markdown","id":"e1184d2e","metadata":{"papermill":{"duration":0.01312,"end_time":"2023-07-08T15:25:14.467107","exception":false,"start_time":"2023-07-08T15:25:14.453987","status":"completed"},"tags":[]},"source":["# 5 | Training Loop 🔁 "]},{"cell_type":"code","execution_count":21,"id":"aaff8c3e","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-08T15:25:14.495886Z","iopub.status.busy":"2023-07-08T15:25:14.495491Z","iopub.status.idle":"2023-07-08T15:25:14.500015Z","shell.execute_reply":"2023-07-08T15:25:14.499038Z"},"papermill":{"duration":0.021531,"end_time":"2023-07-08T15:25:14.502233","exception":false,"start_time":"2023-07-08T15:25:14.480702","status":"completed"},"tags":[]},"outputs":[],"source":["from IPython.display import IFrame"]},{"cell_type":"markdown","id":"1ef7ad2a","metadata":{"papermill":{"duration":0.013159,"end_time":"2023-07-08T15:25:14.529113","exception":false,"start_time":"2023-07-08T15:25:14.515954","status":"completed"},"tags":[]},"source":["# 5.1 | Multi Layer Perceptron Training 🔁\n","\n","Now we will start the training loop \n","\n","I dont the exact reason, but everytime I try to access `GPU` for some training in `Kaggle`. `CUDA goes out of memory`. Thus I have trained the model on `Colab` and will imported the results to `Wandb`.\n","\n","```\n","los = []\n","\n","for epochs in (range(5)):\n","\n","    losses = []\n","\n","    for x , y in tqdm.tqdm(train_dataloader):\n","        \n","        x = torch.tensor(x , dtype = torch.float32)\n","        y = torch.tensor(x , dtype = troch.float32)\n","        \n","        \n","#         x = torch.tensor(x , dtype = torch.float32).to(\"cuda\")\n","#         y = torch.tensor(y , dtype = torch.float32).to(\"cuda\")\n","\n","        optimizer.zero_grad()\n","        preds = MLP(x)\n","\n","        loss = CrossEntropy(preds, y)\n","        losses.append(loss)\n","\n","    los.append(losses)\n","```"]},{"cell_type":"code","execution_count":22,"id":"39617cc8","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-08T15:25:14.557814Z","iopub.status.busy":"2023-07-08T15:25:14.557298Z","iopub.status.idle":"2023-07-08T15:25:14.563415Z","shell.execute_reply":"2023-07-08T15:25:14.562694Z"},"papermill":{"duration":0.022734,"end_time":"2023-07-08T15:25:14.565377","exception":false,"start_time":"2023-07-08T15:25:14.542643","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["\n","        <iframe\n","            width=\"1300\"\n","            height=\"400\"\n","            src=\"https://wandb.ai//ayushsinghal659/CAFA%7CMLP/reports/CAFA-MLP--Vmlldzo0Nzk1NjQx\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","            \n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7a565c3dccd0>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["IFrame(\"https://wandb.ai//ayushsinghal659/CAFA%7CMLP/reports/CAFA-MLP--Vmlldzo0Nzk1NjQx\" , 1300 , 400)"]},{"cell_type":"markdown","id":"55b1ec3f","metadata":{"papermill":{"duration":0.013009,"end_time":"2023-07-08T15:25:14.592064","exception":false,"start_time":"2023-07-08T15:25:14.579055","status":"completed"},"tags":[]},"source":["As we can see we did not get good results, but we will try to omprove our results, by improving the model and by adding new one \n","\n","# 5.2 | Single Layer Perceptron Training 🔁\n","\n","Now we will train our `Single Layer Perceptron`\n","\n","```\n","# torch.cuda.empty_cache()\n","\n","los = []\n","\n","for epochs in (range(4)):\n","\n","    losses = []\n","    \n","    # torch.cuda.empty_cache()\n","    \n","    for x , y in tqdm.tqdm(train_dataloader , total = 10000):\n","\n","        # torch.cuda.empty_cache()\n","\n","        x = torch.tensor(x , dtype = torch.float32)\n","        y = torch.tensor(y , dtype = torch.float32)\n","        \n","        # x = torch.tensor(x , dtype = torch.float32).to(\"cuda\")\n","        # y = torch.tensor(y , dtype = torch.float32).to(\"cuda\")\n","\n","        optimizer.zero_grad()\n","        preds = SLP(x)\n","\n","        loss = CrossEntropy(preds, y)\n","        losses.append(loss)\n","\n","    los.append(losses)\n","```"]},{"cell_type":"code","execution_count":23,"id":"f16cc81c","metadata":{"execution":{"iopub.execute_input":"2023-07-08T15:25:14.620823Z","iopub.status.busy":"2023-07-08T15:25:14.620407Z","iopub.status.idle":"2023-07-08T15:25:14.627076Z","shell.execute_reply":"2023-07-08T15:25:14.626111Z"},"papermill":{"duration":0.023321,"end_time":"2023-07-08T15:25:14.628998","exception":false,"start_time":"2023-07-08T15:25:14.605677","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["\n","        <iframe\n","            width=\"1300\"\n","            height=\"400\"\n","            src=\"https://wandb.ai//ayushsinghal659/uncategorized/reports/CAFA-TEMPROT--Vmlldzo0ODIxNzI3\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","            \n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7a565c3df1f0>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["IFrame(\"https://wandb.ai//ayushsinghal659/uncategorized/reports/CAFA-TEMPROT--Vmlldzo0ODIxNzI3\" , 1300 , 400)"]},{"cell_type":"markdown","id":"e7407f0c","metadata":{"papermill":{"duration":0.013498,"end_time":"2023-07-08T15:25:14.6558","exception":false,"start_time":"2023-07-08T15:25:14.642302","status":"completed"},"tags":[]},"source":["# 6 | TO DO LIST 📄\n","\n","```\n","TO DO 1 : VISUALIZE THE DATA\n","\n","TO DO 2 : TRAIN A MODEL\n","\n","TO DO 3 : TRY DIFFERENT MODELS\n","\n","TO DO 4 : ADD WANDB SUPPORT\n","\n","TO DO 5 : ADD TENSORFLOW DATA LOADER\n","\n","TO DO 6 : TRAIN A TF MODEL\n","\n","TO DO 7 : IMPROVE RESULTS\n","\n","TO DO 8 : DECREASE TRAINING TIME\n","\n","TO DO 9 : DANCE \n","```\n","\n","# 7 | Ending 🏁\n","\n","**THAT'S IT FOR TODAY GUYS**\n","\n","**WE WILL GO DEEPER INTO THE DATA IN THE UPCOMING VERSIONS**\n","\n","**PLEASE COMMENT YOUR THOUGHTS, HIHGLY APPRICIATED**\n","\n","**DONT FORGET TO MAKE AN UPVOTE, IF YOU LIKED MY WORK $:)$**\n","\n","<img src = \"https://i.imgflip.com/19aadg.jpg\">\n","\n","**PEACE OUT $:)$**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":42.617779,"end_time":"2023-07-08T15:25:17.376733","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-08T15:24:34.758954","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}